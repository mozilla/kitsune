

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Using MPI with IPython &mdash; IPython 0.11 documentation</title>
    
    <link rel="stylesheet" href="../_static/default.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.11',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="top" title="IPython 0.11 documentation" href="../index.html" />
    <link rel="up" title="Using IPython for parallel computing" href="index.html" />
    <link rel="next" title="IPython’s Task Database" href="parallel_db.html" />
    <link rel="prev" title="The IPython task interface" href="parallel_task.html" /> 
  </head>
  <body>

<div style="background-color: white; text-align: left; padding: 10px 10px 15px 15px">
<a href="../index.html"><img src="../_static/logo.png" border="0" alt="IPython Documentation"/></a>
</div>

    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="parallel_db.html" title="IPython’s Task Database"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="parallel_task.html" title="The IPython task interface"
             accesskey="P">previous</a> |</li>
        <li><a href="../index.html">home</a>|&nbsp;</li>
        <li><a href="../search.html">search</a>|&nbsp;</li>
       <li><a href="../index.html">documentation </a> &raquo;</li>

          <li><a href="index.html" accesskey="U">Using IPython for parallel computing</a> &raquo;</li> 
      </ul>
    </div>

      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Using MPI with IPython</a><ul>
<li><a class="reference internal" href="#additional-installation-requirements">Additional installation requirements</a></li>
<li><a class="reference internal" href="#starting-the-engines-with-mpi-enabled">Starting the engines with MPI enabled</a><ul>
<li><a class="reference internal" href="#automatic-starting-using-mpiexec-and-ipcluster">Automatic starting using <strong class="command">mpiexec</strong> and <strong class="command">ipcluster</strong></a></li>
<li><a class="reference internal" href="#manual-starting-using-mpiexec">Manual starting using <strong class="command">mpiexec</strong></a></li>
<li><a class="reference internal" href="#automatic-starting-using-pbs-and-ipcluster">Automatic starting using PBS and <strong class="command">ipcluster</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#actually-using-mpi">Actually using MPI</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="parallel_task.html"
                        title="previous chapter">The IPython task interface</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="parallel_db.html"
                        title="next chapter">IPython&#8217;s Task Database</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="../_sources/parallel/parallel_mpi.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" size="18" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="using-mpi-with-ipython">
<span id="parallelmpi"></span><h1>Using MPI with IPython<a class="headerlink" href="#using-mpi-with-ipython" title="Permalink to this headline">¶</a></h1>
<p>Often, a parallel algorithm will require moving data between the engines. One
way of accomplishing this is by doing a pull and then a push using the
multiengine client. However, this will be slow as all the data has to go
through the controller to the client and then back through the controller, to
its final destination.</p>
<p>A much better way of moving data between engines is to use a message passing
library, such as the Message Passing Interface (MPI) <a class="reference internal" href="#mpi">[MPI]</a>. IPython&#8217;s
parallel computing architecture has been designed from the ground up to
integrate with MPI. This document describes how to use MPI with IPython.</p>
<div class="section" id="additional-installation-requirements">
<h2>Additional installation requirements<a class="headerlink" href="#additional-installation-requirements" title="Permalink to this headline">¶</a></h2>
<p>If you want to use MPI with IPython, you will need to install:</p>
<ul class="simple">
<li>A standard MPI implementation such as OpenMPI <a class="reference internal" href="#openmpi">[OpenMPI]</a> or MPICH.</li>
<li>The mpi4py <a class="reference internal" href="#mpi4py">[mpi4py]</a> package.</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The mpi4py package is not a strict requirement. However, you need to
have <em>some</em> way of calling MPI from Python. You also need some way of
making sure that <tt class="xref py py-func docutils literal"><span class="pre">MPI_Init()</span></tt> is called when the IPython engines start
up. There are a number of ways of doing this and a good number of
associated subtleties. We highly recommend just using mpi4py as it
takes care of most of these problems. If you want to do something
different, let us know and we can help you get started.</p>
</div>
</div>
<div class="section" id="starting-the-engines-with-mpi-enabled">
<h2>Starting the engines with MPI enabled<a class="headerlink" href="#starting-the-engines-with-mpi-enabled" title="Permalink to this headline">¶</a></h2>
<p>To use code that calls MPI, there are typically two things that MPI requires.</p>
<ol class="arabic simple">
<li>The process that wants to call MPI must be started using
<strong class="command">mpiexec</strong> or a batch system (like PBS) that has MPI support.</li>
<li>Once the process starts, it must call <tt class="xref py py-func docutils literal"><span class="pre">MPI_Init()</span></tt>.</li>
</ol>
<p>There are a couple of ways that you can start the IPython engines and get
these things to happen.</p>
<div class="section" id="automatic-starting-using-mpiexec-and-ipcluster">
<h3>Automatic starting using <strong class="command">mpiexec</strong> and <strong class="command">ipcluster</strong><a class="headerlink" href="#automatic-starting-using-mpiexec-and-ipcluster" title="Permalink to this headline">¶</a></h3>
<p>The easiest approach is to use the <cite>MPIExec</cite> Launchers in <strong class="command">ipcluster</strong>,
which will first start a controller and then a set of engines using
<strong class="command">mpiexec</strong>:</p>
<div class="highlight-python"><pre>$ ipcluster start --n=4 --elauncher=MPIExecEngineSetLauncher</pre>
</div>
<p>This approach is best as interrupting <strong class="command">ipcluster</strong> will automatically
stop and clean up the controller and engines.</p>
</div>
<div class="section" id="manual-starting-using-mpiexec">
<h3>Manual starting using <strong class="command">mpiexec</strong><a class="headerlink" href="#manual-starting-using-mpiexec" title="Permalink to this headline">¶</a></h3>
<p>If you want to start the IPython engines using the <strong class="command">mpiexec</strong>, just
do:</p>
<div class="highlight-python"><pre>$ mpiexec n=4 ipengine --mpi=mpi4py</pre>
</div>
<p>This requires that you already have a controller running and that the FURL
files for the engines are in place. We also have built in support for
PyTrilinos <a class="reference internal" href="#pytrilinos">[PyTrilinos]</a>, which can be used (assuming is installed) by
starting the engines with:</p>
<div class="highlight-python"><pre>$ mpiexec n=4 ipengine --mpi=pytrilinos</pre>
</div>
</div>
<div class="section" id="automatic-starting-using-pbs-and-ipcluster">
<h3>Automatic starting using PBS and <strong class="command">ipcluster</strong><a class="headerlink" href="#automatic-starting-using-pbs-and-ipcluster" title="Permalink to this headline">¶</a></h3>
<p>The <strong class="command">ipcluster</strong> command also has built-in integration with PBS. For
more information on this approach, see our documentation on <a class="reference internal" href="parallel_process.html#parallel-process"><em>ipcluster</em></a>.</p>
</div>
</div>
<div class="section" id="actually-using-mpi">
<h2>Actually using MPI<a class="headerlink" href="#actually-using-mpi" title="Permalink to this headline">¶</a></h2>
<p>Once the engines are running with MPI enabled, you are ready to go. You can
now call any code that uses MPI in the IPython engines. And, all of this can
be done interactively. Here we show a simple example that uses mpi4py
<a class="reference internal" href="#mpi4py">[mpi4py]</a> version 1.1.0 or later.</p>
<p>First, lets define a simply function that uses MPI to calculate the sum of a
distributed array. Save the following text in a file called <tt class="file docutils literal"><span class="pre">psum.py</span></tt>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">mpi4py</span> <span class="kn">import</span> <span class="n">MPI</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">psum</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">rcvBuf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="s">&#39;d&#39;</span><span class="p">)</span>
    <span class="n">MPI</span><span class="o">.</span><span class="n">COMM_WORLD</span><span class="o">.</span><span class="n">Allreduce</span><span class="p">([</span><span class="n">s</span><span class="p">,</span> <span class="n">MPI</span><span class="o">.</span><span class="n">DOUBLE</span><span class="p">],</span>
        <span class="p">[</span><span class="n">rcvBuf</span><span class="p">,</span> <span class="n">MPI</span><span class="o">.</span><span class="n">DOUBLE</span><span class="p">],</span>
        <span class="n">op</span><span class="o">=</span><span class="n">MPI</span><span class="o">.</span><span class="n">SUM</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">rcvBuf</span>
</pre></div>
</div>
<p>Now, start an IPython cluster:</p>
<div class="highlight-python"><pre>$ ipcluster start --profile=mpi --n=4</pre>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">It is assumed here that the mpi profile has been set up, as described <a class="reference internal" href="parallel_process.html#parallel-process"><em>here</em></a>.</p>
</div>
<p>Finally, connect to the cluster and use this function interactively. In this
case, we create a random array on each engine and sum up all the random arrays
using our <tt class="xref py py-func docutils literal"><span class="pre">psum()</span></tt> function:</p>
<div class="highlight-ipython"><div class="highlight"><pre><span class="gp">In [1]: </span><span class="kn">from</span> <span class="nn">IPython.parallel</span> <span class="kn">import</span> <span class="n">Client</span>

<span class="gp">In [2]: </span><span class="o">%</span><span class="n">load_ext</span> <span class="n">parallel_magic</span>

<span class="gp">In [3]: </span><span class="n">c</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="n">profile</span><span class="o">=</span><span class="s">&#39;mpi&#39;</span><span class="p">)</span>

<span class="gp">In [4]: </span><span class="n">view</span> <span class="o">=</span> <span class="n">c</span><span class="p">[:]</span>

<span class="gp">In [5]: </span><span class="n">view</span><span class="o">.</span><span class="n">activate</span><span class="p">()</span>

<span class="c"># run the contents of the file on each engine:</span>
<span class="gp">In [6]: </span><span class="n">view</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s">&#39;psum.py&#39;</span><span class="p">)</span>

<span class="gp">In [6]: </span><span class="n">px</span> <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="go">Parallel execution on engines: [0,1,2,3]</span>

<span class="gp">In [8]: </span><span class="n">px</span> <span class="n">s</span> <span class="o">=</span> <span class="n">psum</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">Parallel execution on engines: [0,1,2,3]</span>

<span class="gp">In [9]: </span><span class="n">view</span><span class="p">[</span><span class="s">&#39;s&#39;</span><span class="p">]</span>
<span class="gr">Out[9]: </span><span class="p">[</span><span class="mf">187.451545803</span><span class="p">,</span><span class="mf">187.451545803</span><span class="p">,</span><span class="mf">187.451545803</span><span class="p">,</span><span class="mf">187.451545803</span><span class="p">]</span>
</pre></div>
</div>
<p>Any Python code that makes calls to MPI can be used in this manner, including
compiled C, C++ and Fortran libraries that have been exposed to Python.</p>
<table class="docutils citation" frame="void" id="mpi" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[MPI]</a></td><td>Message Passing Interface.  <a class="reference external" href="http://www-unix.mcs.anl.gov/mpi/">http://www-unix.mcs.anl.gov/mpi/</a></td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="mpi4py" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[mpi4py]</td><td><em>(<a class="fn-backref" href="#id3">1</a>, <a class="fn-backref" href="#id5">2</a>)</em> MPI for Python. mpi4py: <a class="reference external" href="http://mpi4py.scipy.org/">http://mpi4py.scipy.org/</a></td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="openmpi" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[OpenMPI]</a></td><td>Open MPI. <a class="reference external" href="http://www.open-mpi.org/">http://www.open-mpi.org/</a></td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="pytrilinos" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id4">[PyTrilinos]</a></td><td>PyTrilinos. <a class="reference external" href="http://trilinos.sandia.gov/packages/pytrilinos/">http://trilinos.sandia.gov/packages/pytrilinos/</a></td></tr>
</tbody>
</table>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="parallel_db.html" title="IPython’s Task Database"
             >next</a> |</li>
        <li class="right" >
          <a href="parallel_task.html" title="The IPython task interface"
             >previous</a> |</li>
        <li><a href="../index.html">home</a>|&nbsp;</li>
        <li><a href="../search.html">search</a>|&nbsp;</li>
       <li><a href="../index.html">documentation </a> &raquo;</li>

          <li><a href="index.html" >Using IPython for parallel computing</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2008, The IPython Development Team.
      Last updated on Jul 31, 2011.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1pre.
    </div>
  </body>
</html>